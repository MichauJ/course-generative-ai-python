{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LanChain basic examples\n",
    "\"Get started\" examples of using LLMs with LangChain framework."
   ],
   "id": "60b3538d446ceb74"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-14T11:23:22.713491Z",
     "start_time": "2024-09-14T11:23:18.467830Z"
    }
   },
   "source": "!pip install openai",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/michal/anaconda3/lib/python3.11/site-packages (1.41.1)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/michal/anaconda3/lib/python3.11/site-packages (from openai) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/michal/anaconda3/lib/python3.11/site-packages (from openai) (1.8.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/michal/anaconda3/lib/python3.11/site-packages (from openai) (0.27.0)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/michal/anaconda3/lib/python3.11/site-packages (from openai) (0.5.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/michal/anaconda3/lib/python3.11/site-packages (from openai) (2.9.1)\r\n",
      "Requirement already satisfied: sniffio in /home/michal/anaconda3/lib/python3.11/site-packages (from openai) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /home/michal/anaconda3/lib/python3.11/site-packages (from openai) (4.66.5)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/michal/anaconda3/lib/python3.11/site-packages (from openai) (4.11.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /home/michal/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\r\n",
      "Requirement already satisfied: certifi in /home/michal/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in /home/michal/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/michal/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/michal/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /home/michal/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\r\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T11:23:22.837138Z",
     "start_time": "2024-09-14T11:23:22.832476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ],
   "id": "5e5e38a0e28b7fc6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T11:23:22.955778Z",
     "start_time": "2024-09-14T11:23:22.943790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def chat(input):\n",
    "    messages = [{\"role\": \"user\", \"content\": input}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "id": "f24064a8fa4ea97b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T11:23:25.599390Z",
     "start_time": "2024-09-14T11:23:23.556152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"Who will fund salary rise for police, teachers, nurses, miners and firefighters?\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Try to answer question as politician\n",
    "Question: {question}\n",
    "\"\"\".format(\n",
    "    question=question\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "chat(prompt)"
   ],
   "id": "235b994c66231c22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Try to answer question as politician\n",
      "Question: Who will fund salary rise for police, teachers, nurses, miners and firefighters?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As a politician, it is our responsibility to ensure that essential workers such as police officers, teachers, nurses, miners, and firefighters are fairly compensated for their hard work and dedication. In order to fund salary increases for these important professions, we must prioritize budget allocations and make strategic decisions on where government resources are allocated. This may involve reevaluating spending in other areas, increasing revenue through taxes or other means, and seeking partnerships with private sector organizations. Ultimately, it is our duty to prioritize the well-being and livelihoods of those who serve our communities, and we will work diligently to find the necessary funding to support their salary increases.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Human message and AI message",
   "id": "d9062c1b83b803a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T11:23:28.775677Z",
     "start_time": "2024-09-14T11:23:27.386074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant! Your name is Janusz.\"),\n",
    "    SystemMessage(content=\"You like pizza with pineapple.\"),\n",
    "    HumanMessage(content=\"What is your name and what pizza do you recommend for today dinner?\"),\n",
    "]\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "print(model.predict_messages(messages))"
   ],
   "id": "7dbff1e3239c95a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16866/4000614462.py:11: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  print(model.predict_messages(messages))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! My name is Janusz. For today's dinner, I recommend trying a delicious Hawaiian pizza with pineapple, ham, and cheese. It's a tasty and satisfying choice!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 47, 'total_tokens': 83, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-6f01517d-2deb-4224-bade-3fb69c2916c4-0' usage_metadata={'input_tokens': 47, 'output_tokens': 36, 'total_tokens': 83}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Wrapper clacces for different LLMs",
   "id": "18733d868932e905"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T10:50:24.927364Z",
     "start_time": "2024-09-14T10:48:37.868852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install --upgrade --quiet langchain-anthropic\n",
    "!pip install --upgrade --quiet langchain-google-genai"
   ],
   "id": "e7343ebfe77bd324",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Antrophic Claude-3",
   "id": "e184110b1c508d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "print(model.invoke(\"What is the area of Australia?\"))"
   ],
   "id": "aa38285e5904cba3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Google Gemini",
   "id": "4a4f230b7e40e2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass()\n",
    "model = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=api_key)\n",
    "print(model.invoke(\"What is the area of Australia?\"))"
   ],
   "id": "c853808ddeb71aa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Deepinfra API: Mixtral, LLama 3.1",
   "id": "90ec37b87040d5f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.llms import DeepInfra\n",
    "\n",
    "os.environ[\"DEEPINFRA_API_TOKEN\"] = '<your DeepInfra API token>'\n",
    "\n",
    "# Create the DeepInfra instance. You can view a list of available parameters in the model page\n",
    "model = DeepInfra(model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = DeepInfra(model_id=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "model.model_kwargs = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"max_new_tokens\": 250,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "print(model.invoke(\"What is the area of Australia?\"))"
   ],
   "id": "973052e3d085f1ae",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
