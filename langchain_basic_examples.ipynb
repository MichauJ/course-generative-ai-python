{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LanChain basic examples\n",
    "\"Get started\" examples of using LLMs with LangChain framework."
   ],
   "id": "60b3538d446ceb74"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-08T09:05:54.360960Z",
     "start_time": "2024-12-08T09:05:52.798577Z"
    }
   },
   "source": "!pip install openai",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/michal/anaconda3/lib/python3.12/site-packages (1.54.1)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from openai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from openai) (2.9.2)\r\n",
      "Requirement already satisfied: sniffio in /home/michal/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /home/michal/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/michal/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /home/michal/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\r\n",
      "Requirement already satisfied: certifi in /home/michal/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /home/michal/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/michal/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/michal/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:05:59.522395Z",
     "start_time": "2024-12-08T09:05:58.994146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ],
   "id": "5e5e38a0e28b7fc6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:06:01.103430Z",
     "start_time": "2024-12-08T09:06:01.079258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def chat(input):\n",
    "    messages = [{\"role\": \"user\", \"content\": input}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "id": "f24064a8fa4ea97b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:06:05.481469Z",
     "start_time": "2024-12-08T09:06:04.252838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"Who will fund salary rise for police, teachers, nurses, miners and firefighters?\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Try to answer question as politician\n",
    "Question: {question}\n",
    "\"\"\".format(\n",
    "    question=question\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "chat(prompt)"
   ],
   "id": "235b994c66231c22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Try to answer question as politician\n",
      "Question: Who will fund salary rise for police, teachers, nurses, miners and firefighters?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As a politician, it is our responsibility to ensure that essential workers such as police officers, teachers, nurses, miners, and firefighters are fairly compensated for their hard work and dedication. In order to fund salary increases for these important professions, we must prioritize budget allocations and make strategic decisions to reallocate resources from other areas. This may involve reevaluating spending priorities, finding efficiencies within government operations, and potentially seeking additional revenue sources through measures such as tax reforms or public-private partnerships. Ultimately, it is crucial that we prioritize the well-being and fair compensation of our essential workers to ensure the continued success and safety of our communities.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Human message and AI message",
   "id": "d9062c1b83b803a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:09:37.144393Z",
     "start_time": "2024-12-08T09:09:35.769783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant! Your name is Janusz.\"),\n",
    "    SystemMessage(content=\"You like pizza with pineapple.\"),\n",
    "    HumanMessage(content=\"What is your name and what pizza do you recommend for today dinner?\"),\n",
    "]\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "print(model.predict_messages(messages))"
   ],
   "id": "7dbff1e3239c95a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41011/4000614462.py:11: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(model.predict_messages(messages))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! My name is Janusz. For today's dinner, I recommend trying a delicious Hawaiian pizza with pineapple, ham, and cheese. It's a classic combination that's sure to satisfy your taste buds!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 47, 'total_tokens': 89, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-dbaeecb6-c931-4132-9492-a30fbfd4d0a0-0' usage_metadata={'input_tokens': 47, 'output_tokens': 42, 'total_tokens': 89, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Wrapper clacces for different LLMs",
   "id": "18733d868932e905"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T18:33:28.228455Z",
     "start_time": "2024-12-07T18:33:17.646627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install --upgrade --quiet langchain-anthropic\n",
    "!pip install --upgrade --quiet langchain-google-genai"
   ],
   "id": "e7343ebfe77bd324",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Antrophic Claude-3",
   "id": "e184110b1c508d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T18:33:48.859001Z",
     "start_time": "2024-12-07T18:33:48.724573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "print(model.invoke(\"What is the area of Australia?\"))"
   ],
   "id": "aa38285e5904cba3",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "\"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_anthropic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChatAnthropic\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m ChatAnthropic(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclaude-3-opus-20240229\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(model\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat is the area of Australia?\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    282\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m    283\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    285\u001B[0m         ChatGeneration,\n\u001B[0;32m--> 286\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[1;32m    287\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[1;32m    288\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    289\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    290\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    291\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    292\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    293\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    294\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    295\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    296\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:786\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    778\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    780\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    784\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    785\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:643\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    642\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 643\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    644\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    645\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    646\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    647\u001B[0m ]\n\u001B[1;32m    648\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:633\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 633\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[1;32m    634\u001B[0m                 m,\n\u001B[1;32m    635\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    636\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    637\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    638\u001B[0m             )\n\u001B[1;32m    639\u001B[0m         )\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:851\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    849\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    850\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 851\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[1;32m    852\u001B[0m             messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    853\u001B[0m         )\n\u001B[1;32m    854\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    855\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_anthropic/chat_models.py:796\u001B[0m, in \u001B[0;36mChatAnthropic._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    794\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m generate_from_stream(stream_iter)\n\u001B[1;32m    795\u001B[0m payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_request_payload(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 796\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mmessages\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpayload)\n\u001B[1;32m    797\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_output(data, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/anthropic/_utils/_utils.py:275\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    273\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 275\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/anthropic/resources/messages.py:888\u001B[0m, in \u001B[0;36mMessages.create\u001B[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m DEPRECATED_MODELS:\n\u001B[1;32m    882\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    883\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe model \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is deprecated and will reach end-of-life on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDEPRECATED_MODELS[model]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    884\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[1;32m    885\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m    886\u001B[0m     )\n\u001B[0;32m--> 888\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[1;32m    889\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/v1/messages\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    890\u001B[0m     body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[1;32m    891\u001B[0m         {\n\u001B[1;32m    892\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[1;32m    893\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[1;32m    894\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[1;32m    895\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    896\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop_sequences\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop_sequences,\n\u001B[1;32m    897\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[1;32m    898\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m: system,\n\u001B[1;32m    899\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[1;32m    900\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[1;32m    901\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[1;32m    902\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_k\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_k,\n\u001B[1;32m    903\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[1;32m    904\u001B[0m         },\n\u001B[1;32m    905\u001B[0m         message_create_params\u001B[38;5;241m.\u001B[39mMessageCreateParams,\n\u001B[1;32m    906\u001B[0m     ),\n\u001B[1;32m    907\u001B[0m     options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[1;32m    908\u001B[0m         extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m    909\u001B[0m     ),\n\u001B[1;32m    910\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mMessage,\n\u001B[1;32m    911\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    912\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mStream[RawMessageStreamEvent],\n\u001B[1;32m    913\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/anthropic/_base_client.py:1279\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1266\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1267\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1274\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1275\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1276\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1277\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1278\u001B[0m     )\n\u001B[0;32m-> 1279\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/anthropic/_base_client.py:956\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    953\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    954\u001B[0m     retries_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 956\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m    957\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m    958\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m    959\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    960\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m    961\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[1;32m    962\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/anthropic/_base_client.py:982\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m    979\u001B[0m options \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_options(options)\n\u001B[1;32m    981\u001B[0m remaining_retries \u001B[38;5;241m=\u001B[39m options\u001B[38;5;241m.\u001B[39mget_max_retries(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries) \u001B[38;5;241m-\u001B[39m retries_taken\n\u001B[0;32m--> 982\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request(options, retries_taken\u001B[38;5;241m=\u001B[39mretries_taken)\n\u001B[1;32m    983\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_request(request)\n\u001B[1;32m    985\u001B[0m kwargs: HttpxSendArgs \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/anthropic/_base_client.py:465\u001B[0m, in \u001B[0;36mBaseClient._build_request\u001B[0;34m(self, options, retries_taken)\u001B[0m\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    463\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected JSON data type, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(json_data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, cannot merge with `extra_body`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 465\u001B[0m headers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_headers(options, retries_taken\u001B[38;5;241m=\u001B[39mretries_taken)\n\u001B[1;32m    466\u001B[0m params \u001B[38;5;241m=\u001B[39m _merge_mappings(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_query, options\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m    467\u001B[0m content_type \u001B[38;5;241m=\u001B[39m headers\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/anthropic/_base_client.py:413\u001B[0m, in \u001B[0;36mBaseClient._build_headers\u001B[0;34m(self, options, retries_taken)\u001B[0m\n\u001B[1;32m    411\u001B[0m custom_headers \u001B[38;5;241m=\u001B[39m options\u001B[38;5;241m.\u001B[39mheaders \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m    412\u001B[0m headers_dict \u001B[38;5;241m=\u001B[39m _merge_mappings(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_headers, custom_headers)\n\u001B[0;32m--> 413\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_headers(headers_dict, custom_headers)\n\u001B[1;32m    415\u001B[0m \u001B[38;5;66;03m# headers are case-insensitive while dictionaries are not.\u001B[39;00m\n\u001B[1;32m    416\u001B[0m headers \u001B[38;5;241m=\u001B[39m httpx\u001B[38;5;241m.\u001B[39mHeaders(headers_dict)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/anthropic/_client.py:189\u001B[0m, in \u001B[0;36mAnthropic._validate_headers\u001B[0;34m(self, headers, custom_headers)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(custom_headers\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAuthorization\u001B[39m\u001B[38;5;124m\"\u001B[39m), Omit):\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 189\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    190\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    191\u001B[0m )\n",
      "\u001B[0;31mTypeError\u001B[0m: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\""
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Google Gemini",
   "id": "4a4f230b7e40e2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T18:35:47.751803Z",
     "start_time": "2024-12-07T18:35:34.154169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass()\n",
    "model = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=api_key)\n",
    "print(model.invoke(\"What is the area of Australia?\"))"
   ],
   "id": "c853808ddeb71aa1",
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "\n  No API_KEY or ADC found. Please either:\n    - Set the `GOOGLE_API_KEY` environment variable.\n    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mDefaultCredentialsError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m api_key \u001B[38;5;241m=\u001B[39m getpass()\n\u001B[1;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m GoogleGenerativeAI(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgemini-pro\u001B[39m\u001B[38;5;124m\"\u001B[39m, google_api_key\u001B[38;5;241m=\u001B[39mapi_key)\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(model\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat is the area of Australia?\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:390\u001B[0m, in \u001B[0;36mBaseLLM.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    387\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    388\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 390\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[1;32m    391\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[1;32m    392\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    393\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    394\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    395\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    396\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    397\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    398\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    399\u001B[0m         )\n\u001B[1;32m    400\u001B[0m         \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    401\u001B[0m         \u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    402\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:755\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    749\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    752\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    753\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    754\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 755\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_strings, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:950\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    935\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    936\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    937\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    938\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    948\u001B[0m         )\n\u001B[1;32m    949\u001B[0m     ]\n\u001B[0;32m--> 950\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_helper(\n\u001B[1;32m    951\u001B[0m         prompts, stop, run_managers, \u001B[38;5;28mbool\u001B[39m(new_arg_supported), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    952\u001B[0m     )\n\u001B[1;32m    953\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[1;32m    954\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:792\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    790\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[1;32m    791\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 792\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    793\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m    794\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:779\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    769\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    770\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    771\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    775\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    776\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    777\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    778\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 779\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[1;32m    780\u001B[0m                 prompts,\n\u001B[1;32m    781\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    782\u001B[0m                 \u001B[38;5;66;03m# TODO: support multiple run managers\u001B[39;00m\n\u001B[1;32m    783\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    784\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    785\u001B[0m             )\n\u001B[1;32m    786\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    787\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    788\u001B[0m         )\n\u001B[1;32m    789\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    790\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_google_genai/llms.py:304\u001B[0m, in \u001B[0;36mGoogleGenerativeAI._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_family \u001B[38;5;241m==\u001B[39m GoogleModelFamily\u001B[38;5;241m.\u001B[39mGEMINI:\n\u001B[0;32m--> 304\u001B[0m         res \u001B[38;5;241m=\u001B[39m _completion_with_retry(\n\u001B[1;32m    305\u001B[0m             \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    306\u001B[0m             prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[1;32m    307\u001B[0m             stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    308\u001B[0m             is_gemini\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    309\u001B[0m             run_manager\u001B[38;5;241m=\u001B[39mrun_manager,\n\u001B[1;32m    310\u001B[0m             generation_config\u001B[38;5;241m=\u001B[39mgeneration_config,\n\u001B[1;32m    311\u001B[0m             safety_settings\u001B[38;5;241m=\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msafety_settings\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    312\u001B[0m         )\n\u001B[1;32m    313\u001B[0m         generation_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    314\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m res\u001B[38;5;241m.\u001B[39musage_metadata \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_google_genai/llms.py:97\u001B[0m, in \u001B[0;36m_completion_with_retry\u001B[0;34m(llm, prompt, is_gemini, stream, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m     94\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocation is not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m exc\u001B[38;5;241m.\u001B[39mmessage:\n\u001B[1;32m     95\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(error_msg)\n\u001B[0;32m---> 97\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _completion_with_retry(\n\u001B[1;32m     98\u001B[0m     prompt\u001B[38;5;241m=\u001B[39mprompt, is_gemini\u001B[38;5;241m=\u001B[39mis_gemini, stream\u001B[38;5;241m=\u001B[39mstream, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m     99\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/tenacity/__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(f, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/tenacity/__init__.py:379\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 379\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter(retry_state\u001B[38;5;241m=\u001B[39mretry_state)\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/tenacity/__init__.py:314\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    312\u001B[0m is_explicit_retry \u001B[38;5;241m=\u001B[39m fut\u001B[38;5;241m.\u001B[39mfailed \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fut\u001B[38;5;241m.\u001B[39mexception(), TryAgain)\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_explicit_retry \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry(retry_state)):\n\u001B[0;32m--> 314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fut\u001B[38;5;241m.\u001B[39mresult()\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter(retry_state)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/concurrent/futures/_base.py:449\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    447\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[1;32m    451\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/tenacity/__init__.py:382\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 382\u001B[0m         result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[1;32m    384\u001B[0m         retry_state\u001B[38;5;241m.\u001B[39mset_exception(sys\u001B[38;5;241m.\u001B[39mexc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_google_genai/llms.py:85\u001B[0m, in \u001B[0;36m_completion_with_retry.<locals>._completion_with_retry\u001B[0;34m(prompt, is_gemini, stream, **kwargs)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_gemini:\n\u001B[0;32m---> 85\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m llm\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mgenerate_content(\n\u001B[1;32m     86\u001B[0m             contents\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[1;32m     87\u001B[0m             stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m     88\u001B[0m             generation_config\u001B[38;5;241m=\u001B[39mgeneration_config,\n\u001B[1;32m     89\u001B[0m             safety_settings\u001B[38;5;241m=\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msafety_settings\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m     90\u001B[0m             request_options\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: llm\u001B[38;5;241m.\u001B[39mtimeout} \u001B[38;5;28;01mif\u001B[39;00m llm\u001B[38;5;241m.\u001B[39mtimeout \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     91\u001B[0m         )\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m llm\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mgenerate_text(prompt\u001B[38;5;241m=\u001B[39mprompt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m google\u001B[38;5;241m.\u001B[39mapi_core\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mFailedPrecondition \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/google/generativeai/generative_models.py:317\u001B[0m, in \u001B[0;36mGenerativeModel.generate_content\u001B[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001B[0m\n\u001B[1;32m    314\u001B[0m     request\u001B[38;5;241m.\u001B[39mcontents[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mrole \u001B[38;5;241m=\u001B[39m _USER_ROLE\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_default_generative_client()\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m request_options \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    320\u001B[0m     request_options \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/google/generativeai/client.py:358\u001B[0m, in \u001B[0;36mget_default_generative_client\u001B[0;34m()\u001B[0m\n\u001B[1;32m    357\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_default_generative_client\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m glm\u001B[38;5;241m.\u001B[39mGenerativeServiceClient:\n\u001B[0;32m--> 358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _client_manager\u001B[38;5;241m.\u001B[39mget_default_client(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerative\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/google/generativeai/client.py:287\u001B[0m, in \u001B[0;36m_ClientManager.get_default_client\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    285\u001B[0m client \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclients\u001B[38;5;241m.\u001B[39mget(name)\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m client \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 287\u001B[0m     client \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_client(name)\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclients[name] \u001B[38;5;241m=\u001B[39m client\n\u001B[1;32m    289\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m client\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/google/generativeai/client.py:247\u001B[0m, in \u001B[0;36m_ClientManager.make_client\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ga_exceptions\u001B[38;5;241m.\u001B[39mDefaultCredentialsError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    241\u001B[0m     e\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m  No API_KEY or ADC found. Please either:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Set the `GOOGLE_API_KEY` environment variable.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    244\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    245\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    246\u001B[0m     )\n\u001B[0;32m--> 247\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_metadata:\n\u001B[1;32m    250\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m client\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/google/generativeai/client.py:239\u001B[0m, in \u001B[0;36m_ClientManager.make_client\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m patch_colab_gce_credentials():\n\u001B[0;32m--> 239\u001B[0m         client \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient_config)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ga_exceptions\u001B[38;5;241m.\u001B[39mDefaultCredentialsError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    241\u001B[0m     e\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m  No API_KEY or ADC found. Please either:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Set the `GOOGLE_API_KEY` environment variable.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    244\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    245\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    246\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:687\u001B[0m, in \u001B[0;36mGenerativeServiceClient.__init__\u001B[0;34m(self, credentials, transport, client_options, client_info)\u001B[0m\n\u001B[1;32m    678\u001B[0m transport_init: Union[\n\u001B[1;32m    679\u001B[0m     Type[GenerativeServiceTransport],\n\u001B[1;32m    680\u001B[0m     Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, GenerativeServiceTransport],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    684\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m cast(Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, GenerativeServiceTransport], transport)\n\u001B[1;32m    685\u001B[0m )\n\u001B[1;32m    686\u001B[0m \u001B[38;5;66;03m# initialize with the provided callable or the passed in class\u001B[39;00m\n\u001B[0;32m--> 687\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transport \u001B[38;5;241m=\u001B[39m transport_init(\n\u001B[1;32m    688\u001B[0m     credentials\u001B[38;5;241m=\u001B[39mcredentials,\n\u001B[1;32m    689\u001B[0m     credentials_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_options\u001B[38;5;241m.\u001B[39mcredentials_file,\n\u001B[1;32m    690\u001B[0m     host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api_endpoint,\n\u001B[1;32m    691\u001B[0m     scopes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_options\u001B[38;5;241m.\u001B[39mscopes,\n\u001B[1;32m    692\u001B[0m     client_cert_source_for_mtls\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_cert_source,\n\u001B[1;32m    693\u001B[0m     quota_project_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_options\u001B[38;5;241m.\u001B[39mquota_project_id,\n\u001B[1;32m    694\u001B[0m     client_info\u001B[38;5;241m=\u001B[39mclient_info,\n\u001B[1;32m    695\u001B[0m     always_use_jwt_access\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    696\u001B[0m     api_audience\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_options\u001B[38;5;241m.\u001B[39mapi_audience,\n\u001B[1;32m    697\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:154\u001B[0m, in \u001B[0;36mGenerativeServiceGrpcTransport.__init__\u001B[0;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001B[0m\n\u001B[1;32m    149\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ssl_channel_credentials \u001B[38;5;241m=\u001B[39m grpc\u001B[38;5;241m.\u001B[39mssl_channel_credentials(\n\u001B[1;32m    150\u001B[0m                 certificate_chain\u001B[38;5;241m=\u001B[39mcert, private_key\u001B[38;5;241m=\u001B[39mkey\n\u001B[1;32m    151\u001B[0m             )\n\u001B[1;32m    153\u001B[0m \u001B[38;5;66;03m# The base transport sets the host, credentials and scopes\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    155\u001B[0m     host\u001B[38;5;241m=\u001B[39mhost,\n\u001B[1;32m    156\u001B[0m     credentials\u001B[38;5;241m=\u001B[39mcredentials,\n\u001B[1;32m    157\u001B[0m     credentials_file\u001B[38;5;241m=\u001B[39mcredentials_file,\n\u001B[1;32m    158\u001B[0m     scopes\u001B[38;5;241m=\u001B[39mscopes,\n\u001B[1;32m    159\u001B[0m     quota_project_id\u001B[38;5;241m=\u001B[39mquota_project_id,\n\u001B[1;32m    160\u001B[0m     client_info\u001B[38;5;241m=\u001B[39mclient_info,\n\u001B[1;32m    161\u001B[0m     always_use_jwt_access\u001B[38;5;241m=\u001B[39malways_use_jwt_access,\n\u001B[1;32m    162\u001B[0m     api_audience\u001B[38;5;241m=\u001B[39mapi_audience,\n\u001B[1;32m    163\u001B[0m )\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_grpc_channel:\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;66;03m# initialize with the provided callable or the default channel\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     channel_init \u001B[38;5;241m=\u001B[39m channel \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreate_channel\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100\u001B[0m, in \u001B[0;36mGenerativeServiceTransport.__init__\u001B[0;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001B[0m\n\u001B[1;32m     96\u001B[0m     credentials, _ \u001B[38;5;241m=\u001B[39m google\u001B[38;5;241m.\u001B[39mauth\u001B[38;5;241m.\u001B[39mload_credentials_from_file(\n\u001B[1;32m     97\u001B[0m         credentials_file, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mscopes_kwargs, quota_project_id\u001B[38;5;241m=\u001B[39mquota_project_id\n\u001B[1;32m     98\u001B[0m     )\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m credentials \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ignore_credentials:\n\u001B[0;32m--> 100\u001B[0m     credentials, _ \u001B[38;5;241m=\u001B[39m google\u001B[38;5;241m.\u001B[39mauth\u001B[38;5;241m.\u001B[39mdefault(\n\u001B[1;32m    101\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mscopes_kwargs, quota_project_id\u001B[38;5;241m=\u001B[39mquota_project_id\n\u001B[1;32m    102\u001B[0m     )\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001B[39;00m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(credentials, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith_gdch_audience\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/google/auth/_default.py:697\u001B[0m, in \u001B[0;36mdefault\u001B[0;34m(scopes, request, quota_project_id, default_scopes)\u001B[0m\n\u001B[1;32m    689\u001B[0m             _LOGGER\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m    690\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo project ID could be determined. Consider running \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    691\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`gcloud config set project` or setting the \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    692\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment variable\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    693\u001B[0m                 environment_vars\u001B[38;5;241m.\u001B[39mPROJECT,\n\u001B[1;32m    694\u001B[0m             )\n\u001B[1;32m    695\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m credentials, effective_project_id\n\u001B[0;32m--> 697\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001B[0;31mDefaultCredentialsError\u001B[0m: \n  No API_KEY or ADC found. Please either:\n    - Set the `GOOGLE_API_KEY` environment variable.\n    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information."
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Deepinfra API: Mixtral, LLama 3.1",
   "id": "90ec37b87040d5f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T18:35:25.515364Z",
     "start_time": "2024-12-07T18:35:24.733268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.llms import DeepInfra\n",
    "import os\n",
    "\n",
    "os.environ[\"DEEPINFRA_API_TOKEN\"] = '<your DeepInfra API token>'\n",
    "\n",
    "# Create the DeepInfra instance. You can view a list of available parameters in the model page\n",
    "model = DeepInfra(model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = DeepInfra(model_id=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "model.model_kwargs = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"max_new_tokens\": 250,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "print(model.invoke(\"What is the area of Australia?\"))"
   ],
   "id": "973052e3d085f1ae",
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "DeepInfra Server: Unauthorized",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 16\u001B[0m\n\u001B[1;32m      8\u001B[0m model \u001B[38;5;241m=\u001B[39m DeepInfra(model_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmistralai/Mistral-7B-Instruct-v0.3\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m model\u001B[38;5;241m.\u001B[39mmodel_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.7\u001B[39m,\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepetition_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1.2\u001B[39m,\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_new_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m250\u001B[39m,\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.9\u001B[39m,\n\u001B[1;32m     14\u001B[0m }\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28mprint\u001B[39m(model\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat is the area of Australia?\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:390\u001B[0m, in \u001B[0;36mBaseLLM.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    387\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    388\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 390\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[1;32m    391\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[1;32m    392\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    393\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    394\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    395\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    396\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    397\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    398\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    399\u001B[0m         )\n\u001B[1;32m    400\u001B[0m         \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    401\u001B[0m         \u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    402\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:755\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    749\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    752\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    753\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    754\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 755\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_strings, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:950\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    935\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    936\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    937\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    938\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    948\u001B[0m         )\n\u001B[1;32m    949\u001B[0m     ]\n\u001B[0;32m--> 950\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_helper(\n\u001B[1;32m    951\u001B[0m         prompts, stop, run_managers, \u001B[38;5;28mbool\u001B[39m(new_arg_supported), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    952\u001B[0m     )\n\u001B[1;32m    953\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[1;32m    954\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:792\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    790\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[1;32m    791\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 792\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    793\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m    794\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:779\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    769\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    770\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    771\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    775\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    776\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    777\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    778\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 779\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[1;32m    780\u001B[0m                 prompts,\n\u001B[1;32m    781\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    782\u001B[0m                 \u001B[38;5;66;03m# TODO: support multiple run managers\u001B[39;00m\n\u001B[1;32m    783\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    784\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    785\u001B[0m             )\n\u001B[1;32m    786\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    787\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    788\u001B[0m         )\n\u001B[1;32m    789\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    790\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1502\u001B[0m, in \u001B[0;36mLLM._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1499\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1500\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[1;32m   1501\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m-> 1502\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1503\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m   1504\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1505\u001B[0m     )\n\u001B[1;32m   1506\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_community/llms/deepinfra.py:129\u001B[0m, in \u001B[0;36mDeepInfra._call\u001B[0;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    126\u001B[0m request \u001B[38;5;241m=\u001B[39m Requests(headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_headers())\n\u001B[1;32m    127\u001B[0m response \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39mpost(url\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_url(), data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_body(prompt, kwargs))\n\u001B[0;32m--> 129\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_status(response\u001B[38;5;241m.\u001B[39mstatus_code, response\u001B[38;5;241m.\u001B[39mtext)\n\u001B[1;32m    130\u001B[0m data \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_community/llms/deepinfra.py:89\u001B[0m, in \u001B[0;36mDeepInfra._handle_status\u001B[0;34m(self, code, text)\u001B[0m\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDeepInfra Server: Error \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m401\u001B[39m:\n\u001B[0;32m---> 89\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDeepInfra Server: Unauthorized\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDeepInfra Server: Unauthorized\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mException\u001B[0m: DeepInfra Server: Unauthorized"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
