{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformers library use case types\n",
    "Common examples of using transformers library with models from huggingface.co."
   ],
   "id": "7628049b1250bae2"
  },
  {
   "cell_type": "code",
   "id": "44508dfc-47c4-4a3e-9a11-f99f4fb0490d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T06:04:54.081924Z",
     "start_time": "2024-11-10T06:01:01.820493Z"
    }
   },
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow\n",
    "!pip install tf-keras\n",
    "!pip install sacremoses\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\r\n",
      "Requirement already satisfied: filelock in /home/michal/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\r\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\r\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/michal/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/michal/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\r\n",
      "Requirement already satisfied: requests in /home/michal/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\r\n",
      "Collecting safetensors>=0.4.1 (from transformers)\r\n",
      "  Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\r\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\r\n",
      "  Using cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/michal/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/michal/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\r\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.0/10.0 MB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\r\n",
      "Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\r\n",
      "Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\r\n",
      "Successfully installed huggingface-hub-0.26.2 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.2\r\n",
      "Collecting tensorflow\r\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\r\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\r\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\r\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\r\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\r\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\r\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\r\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\r\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\r\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\r\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\r\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Requirement already satisfied: packaging in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.1)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.25.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow) (75.1.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\r\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\r\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\r\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\r\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\r\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\r\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting keras>=3.5.0 (from tensorflow)\r\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\r\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\r\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\r\n",
      "Requirement already satisfied: rich in /home/michal/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\r\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\r\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\r\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\r\n",
      "  Downloading optree-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\r\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\r\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\r\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m615.5/615.5 MB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:03\u001B[0m\r\n",
      "\u001B[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\r\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\r\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\r\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.9/5.9 MB\u001B[0m \u001B[31m5.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.5/24.5 MB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\r\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.5/5.5 MB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\r\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/6.6 MB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\r\n",
      "Downloading optree-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (362 kB)\r\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\r\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.67.1 keras-3.6.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0\r\n",
      "Collecting tf-keras\r\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /home/michal/anaconda3/lib/python3.12/site-packages (from tf-keras) (2.18.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.1)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.25.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.11.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.67.1)\r\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\r\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.6.0)\r\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\r\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\r\n",
      "Requirement already satisfied: rich in /home/michal/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\r\n",
      "Requirement already satisfied: namex in /home/michal/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\r\n",
      "Requirement already satisfied: optree in /home/michal/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/michal/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2024.8.30)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/michal/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/michal/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\r\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tf-keras\r\n",
      "Successfully installed tf-keras-2.18.0\r\n",
      "Collecting sacremoses\r\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Requirement already satisfied: regex in /home/michal/anaconda3/lib/python3.12/site-packages (from sacremoses) (2024.9.11)\r\n",
      "Requirement already satisfied: click in /home/michal/anaconda3/lib/python3.12/site-packages (from sacremoses) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /home/michal/anaconda3/lib/python3.12/site-packages (from sacremoses) (1.4.2)\r\n",
      "Requirement already satisfied: tqdm in /home/michal/anaconda3/lib/python3.12/site-packages (from sacremoses) (4.66.5)\r\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m897.5/897.5 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: sacremoses\r\n",
      "Successfully installed sacremoses-0.1.1\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "3eb54b97-b0e3-4a24-a907-48a8778dbbfe",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "id": "45e033c3-383b-4896-94e2-fb39ca81f6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T06:05:15.298658Z",
     "start_time": "2024-11-10T06:04:54.349283Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline\n",
    "\n",
    "def download_model(model_name: str, pipe_type='text-generation'):\n",
    "   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "   model = AutoModelWithLMHead.from_pretrained(model_name)\n",
    "   pipe = pipeline(pipe_type, model=model, tokenizer=tokenizer)\n",
    "   return pipe\n",
    "\n",
    "\n",
    "text = \"Provide recipe with list of steps to cook cheesecake.\"\n",
    "pipe = download_model(\"gpt2\")\n",
    "text_generated = pipe(text, max_new_tokens=200,temperature=0.01)\n",
    "\n",
    "print(text_generated[0]['generated_text'])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 07:04:56.281724: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-10 07:04:56.289057: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-10 07:04:56.310373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731218696.345070   14270 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731218696.355540   14270 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-10 07:04:56.414004: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecc5e584837e44d8956e2724d43ace66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1266127ab1d4a2e960990bbbc32fd9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90063004865d4377b7b51c0e4c939101"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1469d9ef94b041a2a1cd188046a76fc5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c36be4ba2cd44d0988e6c2a05f8e4ec8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelWithLMHead requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelWithLMHead\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 11\u001B[0m\n\u001B[1;32m      7\u001B[0m    \u001B[38;5;28;01mreturn\u001B[39;00m pipe\n\u001B[1;32m     10\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProvide recipe with list of steps to cook cheesecake.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 11\u001B[0m pipe \u001B[38;5;241m=\u001B[39m download_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m text_generated \u001B[38;5;241m=\u001B[39m pipe(text, max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m,temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(text_generated[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m, in \u001B[0;36mdownload_model\u001B[0;34m(model_name, pipe_type)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdownload_model\u001B[39m(model_name: \u001B[38;5;28mstr\u001B[39m, pipe_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m      4\u001B[0m    tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[0;32m----> 5\u001B[0m    model \u001B[38;5;241m=\u001B[39m AutoModelWithLMHead\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[1;32m      6\u001B[0m    pipe \u001B[38;5;241m=\u001B[39m pipeline(pipe_type, model\u001B[38;5;241m=\u001B[39mmodel, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer)\n\u001B[1;32m      7\u001B[0m    \u001B[38;5;28;01mreturn\u001B[39;00m pipe\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1651\u001B[0m, in \u001B[0;36mDummyObject.__getattribute__\u001B[0;34m(cls, key)\u001B[0m\n\u001B[1;32m   1649\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_from_config\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1650\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(key)\n\u001B[0;32m-> 1651\u001B[0m requires_backends(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_backends)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1630\u001B[0m, in \u001B[0;36mrequires_backends\u001B[0;34m(obj, backends)\u001B[0m\n\u001B[1;32m   1628\u001B[0m \u001B[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001B[39;00m\n\u001B[1;32m   1629\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m is_tf_available():\n\u001B[0;32m-> 1630\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001B[38;5;241m.\u001B[39mformat(name))\n\u001B[1;32m   1632\u001B[0m \u001B[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001B[39;00m\n\u001B[1;32m   1633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tf_available():\n",
      "\u001B[0;31mImportError\u001B[0m: \nAutoModelWithLMHead requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelWithLMHead\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "d75a3c63-9a94-433e-8617-46eb67a198d2",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d1673c1-841d-4d5f-bc5b-26a8d8a42cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: The government announced new tax reforms today.\n",
      "Predicted Category: Polityka, Confidence: 0.31741610169410706\n",
      "\n",
      "Article: The local team won the championship in a thrilling match.\n",
      "Predicted Category: Sport, Confidence: 0.6169815063476562\n",
      "\n",
      "Article: New advancements in AI are reshaping the tech industry.\n",
      "Predicted Category: Technologia, Confidence: 0.6335176825523376\n",
      "\n",
      "Article: The art exhibit showcased contemporary works by emerging artists.\n",
      "Predicted Category: Kultura, Confidence: 0.2816486954689026\n",
      "\n",
      "Article: New guidelines for a healthy diet were published by the health department.\n",
      "Predicted Category: Polityka, Confidence: 0.2837600111961365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Załaduj model klasyfikacji tekstów\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Przykładowe artykuły\n",
    "articles = [\n",
    "    \"The government announced new tax reforms today.\",\n",
    "    \"The local team won the championship in a thrilling match.\",\n",
    "    \"New advancements in AI are reshaping the tech industry.\",\n",
    "    \"The art exhibit showcased contemporary works by emerging artists.\",\n",
    "    \"New guidelines for a healthy diet were published by the health department.\"\n",
    "]\n",
    "\n",
    "# Definiowanie możliwych kategorii\n",
    "candidate_labels = [\"Polityka\", \"Sport\", \"Technologia\", \"Kultura\", \"Zdrowie\"]\n",
    "\n",
    "# Klasyfikacja artykułów\n",
    "results = classifier(articles, candidate_labels=candidate_labels)\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "for article, result in zip(articles, results):\n",
    "    print(f\"Article: {article}\")\n",
    "    print(f\"Predicted Category: {result['labels'][0]}, Confidence: {result['scores'][0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048dcf8e-e246-4bab-bf49-0a2637722e08",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5937a6ce-2c2f-413b-9965-fd26b48ea123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1ce0d3c3bc42c5b22e422fc103896b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7319c16117741aca80e26d5275ac6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ffec19dd87432290e666082ae620cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a52e79a91b640a193726482a9f1fae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec31174161649d1bcaf9a467dc6f975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This product is amazing! I loved it.\n",
      "Sentiment: positive, Confidence: 0.9866390228271484\n",
      "\n",
      "Review: I am very disappointed. The product broke after one use.\n",
      "Sentiment: negative, Confidence: 0.930964469909668\n",
      "\n",
      "Review: It's okay, does the job but nothing special.\n",
      "Sentiment: neutral, Confidence: 0.6142873764038086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Załaduj model klasyfikacji sentymentu (może to być np. model GPT lub inny dostępny w ramach transformers)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# Przykładowe recenzje\n",
    "reviews = [\n",
    "    \"This product is amazing! I loved it.\",\n",
    "    \"I am very disappointed. The product broke after one use.\",\n",
    "    \"It's okay, does the job but nothing special.\"\n",
    "]\n",
    "\n",
    "# Klasyfikacja recenzji\n",
    "results = classifier(reviews)\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "for review, result in zip(reviews, results):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {result['label']}, Confidence: {result['score']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4b4e3-4f4a-46f2-8a00-21d5ca5e3c3d",
   "metadata": {},
   "source": [
    "## Document analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0824601-ebaa-4076-9960-908f9acdc921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410a2c61fa344bf7acaf8e18a838434f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at google/flan-t5-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0001070289799827151, 'start': 10720, 'end': 10760, 'answer': ' <td style=\"text-align:right;\">45,3</td>'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=\"google/flan-t5-base\")\n",
    "\n",
    "file = open(\"data/annual_report.html\", \"r\")\n",
    "document = file.read()\n",
    "result = pipe(question=\"What is annual revenue of the company based on attached annual report?\", context=document )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b660b8-7bcb-4ca6-bc2b-11a6f257f292",
   "metadata": {},
   "source": [
    "## Machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fca90d40-1524-4a85-a955-3f812e620f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-en-de and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Ich bin Ausländer und spreche kein fließend Deutsch.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation\", model=\"facebook/wmt19-en-de\")\n",
    "\n",
    "result = pipe(\"I'm foreigner and I don't speak german fluently.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c066a83-65fb-41d9-8962-b2bd9495c1af",
   "metadata": {},
   "source": [
    "## Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa8dfd1f-1de7-45b8-9056-8934f5dd05b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.2113601565361023, 'start': 19, 'end': 40, 'answer': 'z siedzibą w Poznaniu'}, {'score': 0.09133698046207428, 'start': 350, 'end': 373, 'answer': 'Marketing i technologie'}, {'score': 0.00805636402219534, 'start': 1058, 'end': 1062, 'answer': '1996'}, {'score': 0.0014388621784746647, 'start': 50, 'end': 82, 'answer': 'uczelnia niepubliczna w Poznaniu'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "result = pipe(context=\"\"\"Collegium Da Vinci z siedzibą w Poznaniu – polska uczelnia niepubliczna w Poznaniu. Collegium Da Vinci to praktyczna uczelnia biznesowa kształcąca specjalistów w obszarze kreatywnego sektora biznesu. W ofercie posiada studia dyplomowe (I i II stopnia) i podyplomowe (w tym EMBA) w 4 głównych obszarach: Media kreatywne i sztuka, IT i analiza danych, Marketing i technologie, Zarządzanie i HR.\n",
    "Uczelnia realizuje model nauczania oparty na diagnozie indywidualnych predyspozycji studentów (test Gallupa, Insightful Profiler™), indywidualizacji ścieżek kształcenia (moduły dodatkowe) oraz wsparciu tutorów w wyznaczaniu i osiąganiu edukacyjnych celów.\n",
    "Oferowane przez CDV kierunki są mocno osadzone w realiach rynkowych, są tworzone wspólnie z biznesem oraz zorientowane na naukę praktycznych umiejętności zawodowych, aby pomagać studentom i słuchaczom w zaprojektowaniu lub rozwoju ich kariery zawodowej. Collegium Da Vinci kształci w podejściu interdyscyplinarnym, łącząc biznes z kreatywnością i humanistycznym podejściem do technologii.\n",
    "Historia\n",
    "10 czerwca 1996 Minister Edukacji Narodowej udzielił zezwolenia na utworzenie Wyższej Szkoły Nauk Humanistycznych i Dziennikarstwa w Poznaniu, której pomysłodawcą oraz założycielem jest Piotr Voelkel. WSNHiD zostało wpisane do wykazu uczelni niepublicznych pod numerem 90. W tym samym roku pierwsi studenci rozpoczęli kształcenie na kierunku politologia i nauki społeczne. Dwa lata później na WSNHiD powstaje nowy kierunek – stosunki międzynarodowe. W 2000 uruchomione zostają kolejne kierunki – socjologia i kulturoznawstwo, a w roku następnym – informatyka. W 2006 uczelnia uzyskuje zgodę na kształcenie na kierunku pedagogika. \"\"\",\n",
    "      question=['What is Collegium da Vinci?',\n",
    "'What are services of Collegium da Vinci?',\n",
    "'When Collegium Da Vinci was founded?',\n",
    "'Where are headquarters of Collegium da Vinci?'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c5c6e-7904-4b10-aabe-0480ab29e975",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e6b83d-a125-4a58-9aa4-9f86380fb521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Dzień był letni i świąteczny. Ciepło i radość lały się z błękitnego nieba i złotego słońca.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "file = open(\"data/nad_niemnem.txt\", \"r\")\n",
    "document = file.read()\n",
    "result = pipe(document[:800], max_length=100, min_length=20, do_sample=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247af2a0-5321-404c-9849-212ad552ee26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
